{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKDOCLP+fQEF1i93KfcCi9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkech/RAG_Workshop/blob/main/Google_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique that enhances the performance of language models by integrating information retrieval mechanisms with generative capabilities. This approach allows the model to generate more accurate and contextually relevant responses by leveraging external knowledge sources.\n",
        "\n",
        "![RAG Schema](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jy3OIYsIi9NcsDsfaNC_6w.png)\n",
        "\n",
        "\n",
        "## How RAG Works\n",
        "\n",
        "The RAG process can be broken down into the following steps, as illustrated in the schema:\n",
        "\n",
        "1. **Documents Preparation:**\n",
        "   - **Chunked Texts:** The input documents are first divided into smaller, manageable chunks of text. This step ensures that the retrieval process can handle large documents effectively.\n",
        "\n",
        "2. **Embeddings Generation:**\n",
        "   - **Generate Embeddings:** Each chunked text is then converted into embeddings using a suitable embedding model. These embeddings capture the semantic meaning of the text and are essential for the retrieval process.\n",
        "\n",
        "3. **Vector Database:**\n",
        "   - **Store Embeddings:** The generated embeddings are stored in a vector database. This database allows efficient similarity search to find relevant text passages based on the input prompt.\n",
        "\n",
        "4. **Prompt Handling:**\n",
        "   - **Prompt Embedding:** When a prompt is received, it is also converted into an embedding using the same or a similar embedding model used for the documents.\n",
        "\n",
        "5. **Retrieval Phase:**\n",
        "   - **Retrieve Relevant Passages:** The prompt embedding is used to search the vector database for the most relevant text passages. These passages provide the context needed for generating a response.\n",
        "\n",
        "6. **Prompt + Context:**\n",
        "   - **Combine Prompt and Context:** The retrieved relevant text passages (context) are combined with the original prompt. This combined input is then fed into the language model.\n",
        "\n",
        "7. **Generation Phase:**\n",
        "   - **LLM (Large Language Model):** The language model processes the combined prompt and context to generate a response. The integration of retrieved context helps the model produce more accurate and informed outputs.\n",
        "\n",
        "8. **Result:**\n",
        "   - **Output:** The generated response is returned as the final result, incorporating the relevant information retrieved from the external knowledge sources.\n",
        "\n",
        "## Benefits of RAG\n",
        "\n",
        "- **Improved Accuracy:** By incorporating relevant external information, RAG enhances the accuracy of the generated responses.\n",
        "- **Contextual Relevance:** The retrieval phase ensures that the generated text is contextually relevant, addressing specific queries more effectively.\n",
        "- **Knowledge Integration:** RAG allows the integration of up-to-date information from external sources, making it suitable for dynamic and information-rich tasks.\n",
        "\n",
        "RAG provides a powerful framework for combining the strengths of information retrieval and text generation, enabling the creation of more robust and context-aware language models.\n"
      ],
      "metadata": {
        "id": "EFAg5EKS7Zs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Augmented Generation (RAG)\n",
        "![RAG](https://drive.google.com/uc?id=1Y6IT6wKrDwjOIX-3IZlq5LVlHRcEHebC)"
      ],
      "metadata": {
        "id": "pw3LrZzrxSvG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pkr3NMUaxMpL"
      },
      "outputs": [],
      "source": [
        "!pip install datasets pymongo sentence_transformers gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset\n",
        "![Dataset loading](https://drive.google.com/uc?id=1fC7fCFRxfgzC1sfemaJEK5fGlk3yqqpY)"
      ],
      "metadata": {
        "id": "RzvZ3xK6xnJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ashmib/wikivoyage-eu-city-embeddings\", download_mode=\"force_redownload\") ## downloading it from HuggingFace\n",
        "dataset.set_format(type='pandas') ## converting it into pandas\n",
        "df = dataset[\"train\"][:]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nwy30YZ5xSjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embed Model\n",
        "![Embedding.png](https://drive.google.com/uc?id=1yMntmFJdugi1ib-JNOoqlo_WbPgWvDy3)"
      ],
      "metadata": {
        "id": "wNlaB3P5xwDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "\n",
        "    embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
        "    if not text.strip():\n",
        "        print(\"Attempted to get embedding for empty text.\")\n",
        "        return []\n",
        "\n",
        "    embedding = embedding_model.encode(text)\n",
        "\n",
        "    return embedding.tolist()"
      ],
      "metadata": {
        "id": "nqsql7V6x1cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Ingestion & Vector Database Setup"
      ],
      "metadata": {
        "id": "3O9sV3AQx3uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_mongo_url():\n",
        "    username = userdata.get(\"MONGO_USERNAME\")\n",
        "    password = userdata.get(\"MONGO_PW\")\n",
        "    # uri_string = \"@cluster0.62unmco.mongodb.net/\"\n",
        "    uri_string = userdata.get(\"MONGO_CONN_STR\")\n",
        "    mongo_url = f\"mongodb+srv://{username}:{password}{uri_string}\"\n",
        "    return mongo_url\n",
        "\n",
        "\n",
        "def get_mongo_client(mongo_url):\n",
        "    \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "    if not mongo_url:\n",
        "        print(\"MONGO_URI not set in environment variables\")\n",
        "    try:\n",
        "        client = pymongo.MongoClient(mongo_url)\n",
        "        print(\"Connection to MongoDB successful\")\n",
        "        return client\n",
        "    except pymongo.errors.ConnectionFailure as e:\n",
        "        print(f\"Connection failed: {e}\")\n",
        "        return None\n",
        "\n",
        "## Data ingestion in Mongodb\n",
        "\n",
        "mongo_url = get_mongo_url()\n",
        "if not mongo_url:\n",
        "    print(\"MONGO_creds not set in environment variables\")\n",
        "\n",
        "# establishes database connection\n",
        "mongo_client = get_mongo_client(mongo_url)\n",
        "# creates database\n",
        "db = mongo_client[\"wikivoyage_cities\"]\n",
        "# creates collection\n",
        "collection = db[\"wikivoyage_collection_2\"]\n",
        "\n",
        "# Delete any existing records in the collection\n",
        "collection.delete_many({})\n",
        "\n",
        "# data ingestion into mongoDB\n",
        "documents = df.to_dict('records')\n",
        "collection.insert_many(documents)\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ],
      "metadata": {
        "id": "w9Ss7EKXx9v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Retrieval based on context\n",
        "![Retrieval](https://drive.google.com/uc?id=1Qnk-v5ZGFo1DN3Ye6iylcL4hOL9ATHjR)"
      ],
      "metadata": {
        "id": "vGUnr3Y1yHo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_results(query, mongo_url):\n",
        "    mongo_client = get_mongo_client(mongo_url)\n",
        "    db = mongo_client[\"wikivoyage_cities\"]\n",
        "\n",
        "    query_embedding = get_embedding(query)\n",
        "    results = db.wikivoyage_collection_2.aggregate([\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_embedding,\n",
        "                \"numCandidates\": 150,\n",
        "                \"limit\": 5\n",
        "            }\n",
        "        }\n",
        "    ])\n",
        "    return results\n",
        "\n",
        "def get_search_result(query, mongo_url):\n",
        "    get_knowledge = query_results(query, mongo_url)\n",
        "    print(get_knowledge)\n",
        "\n",
        "    search_result = \"\"\n",
        "    for result in get_knowledge:\n",
        "        search_result += f\"City: {result.get('city', 'N/A')}, Abstract: {result.get('combined', 'N/A')}\\n\"\n",
        "\n",
        "    return search_result"
      ],
      "metadata": {
        "id": "cgb6oUyMyOMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation and Generation\n",
        "![Augmentation & Generation](https://drive.google.com/uc?id=11-Hn2WbD8_KqsQpxny9ovH1IZa78r7PW)\n"
      ],
      "metadata": {
        "id": "_siB5PObyVBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "HF_token = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "def generate_text(query, model_name: str | None = \"google/gemma-2b-it\"):\n",
        "    if model_name is None:\n",
        "        model_name = \"google/gemma-2b-it\"\n",
        "\n",
        "    # establish mongo connection\n",
        "    mongo_url = get_mongo_url()\n",
        "\n",
        "    # get the top 5 most similar documents from the Vector data base\n",
        "    source_information = get_search_result(query, mongo_url)\n",
        "\n",
        "    # augment the query with the context\n",
        "    combined_information = (\n",
        "        f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
        "    )\n",
        "\n",
        "    # use the HF inference client to generate the text\n",
        "    client = InferenceClient(model_name, token=HF_token)\n",
        "    stream = client.text_generation(prompt=combined_information, details=True, stream=True, max_new_tokens=2048,\n",
        "                                    return_full_text=False)\n",
        "   # formatting the output\n",
        "    output = \"\"\n",
        "\n",
        "    for response in stream:\n",
        "        output += response.token.text\n",
        "\n",
        "    if \"<eos>\" in output:\n",
        "        output = output.split(\"<eos>\")[0]\n",
        "    return output\n",
        "\n",
        "query = \"I am planning a vacation to Spain. Can you suggest a one-week itinerary including must-visit places and local cuisines to try?\"\n",
        "generate_text(query)"
      ],
      "metadata": {
        "id": "Sx6xthaRyZzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating UI using [Gradio](https://www.gradio.app/docs)"
      ],
      "metadata": {
        "id": "ZjS9sDnKykJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "examples = [[\"I'm planning a vacation to France. Can you suggest a one-week itinerary including must-visit places and \"\n",
        "             \"local cuisines to try?\", None],\n",
        "            [\"Recommend places that are similar to Istanbul in terms of architecture\", None],\n",
        "            ]\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\"text\",\n",
        "            gr.Dropdown(\n",
        "                [\"google/gemma-2b-it\",], label=\"Models\", info=\"Will \"\n",
        "                                                                                                             \"add \"\n",
        "                                                                                                             \"more \"\n",
        "                                                                                                             \"models \"\n",
        "                                                                                                             \"later! \"\n",
        "            ),\n",
        "            ],\n",
        "    title=\"🇪🇺 Euro City TravelBot 🇪🇺\",\n",
        "    description=\"Travel related queries for Europe.\",\n",
        "    outputs=[\"text\"],\n",
        "    examples=examples,\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "9fGz1oaMynOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}